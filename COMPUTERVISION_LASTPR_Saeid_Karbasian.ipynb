{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Ù…ÙˆØªÙˆØ± Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø§ ØªØ­Ù„ÛŒÙ„ Ù¾ÛŒØ´Ø±ÙØªÙ‡**"
      ],
      "metadata": {
        "id": "20xx4fvJjQBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ù†ØµØ¨ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²\n",
        "!pip install -q faiss-cpu torch torchvision transformers gradio pillow requests tqdm matplotlib seaborn scikit-learn\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import faiss\n",
        "import gradio as gr\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Import models\n",
        "from torchvision import models, transforms\n",
        "from transformers import CLIPModel, CLIPProcessor, ViTModel, ViTImageProcessor\n",
        "\n",
        "class MultiModalImageSearchEngine:\n",
        "    \"\"\"Ù…ÙˆØªÙˆØ± Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØµÙˆÛŒØ±ÛŒ Ú†Ù†Ø¯ Ù…Ø¯Ù„Ù‡\"\"\"\n",
        "\n",
        "    def __init__(self, model_types=['resnet50', 'vit', 'clip']):\n",
        "        self.model_types = model_types\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.models = {}\n",
        "        self.processors = {}\n",
        "        self.transforms = {}\n",
        "        self.feature_dims = {}\n",
        "        self.indices = {}\n",
        "        self.image_paths = []\n",
        "        self.features = {}\n",
        "\n",
        "        print(f\"ğŸš€ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±ÙˆÛŒ {self.device}...\")\n",
        "        self._load_all_models()\n",
        "\n",
        "    def _load_all_models(self):\n",
        "        \"\"\"Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§\"\"\"\n",
        "\n",
        "        # 1. ResNet50 (CNN)\n",
        "        if 'resnet50' in self.model_types:\n",
        "            print(\"ğŸ“¦ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ResNet50...\")\n",
        "            self.models['resnet50'] = models.resnet50(pretrained=True)\n",
        "            self.models['resnet50'] = torch.nn.Sequential(*list(self.models['resnet50'].children())[:-1])\n",
        "            self.models['resnet50'].to(self.device)\n",
        "            self.models['resnet50'].eval()\n",
        "            self.feature_dims['resnet50'] = 2048\n",
        "\n",
        "            self.transforms['resnet50'] = transforms.Compose([\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224, 0.225])\n",
        "            ])\n",
        "\n",
        "        # 2. Vision Transformer (ViT)\n",
        "        if 'vit' in self.model_types:\n",
        "            print(\"ğŸ“¦ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ViT...\")\n",
        "            self.processors['vit'] = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
        "            self.models['vit'] = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
        "            self.models['vit'].to(self.device)\n",
        "            self.models['vit'].eval()\n",
        "            self.feature_dims['vit'] = 768\n",
        "\n",
        "        # 3. CLIP\n",
        "        if 'clip' in self.model_types:\n",
        "            print(\"ğŸ“¦ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ CLIP...\")\n",
        "            self.models['clip'] = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "            self.processors['clip'] = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "            self.models['clip'].to(self.device)\n",
        "            self.models['clip'].eval()\n",
        "            self.feature_dims['clip'] = 512\n",
        "\n",
        "        print(\"âœ… Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù†Ø¯!\")\n",
        "\n",
        "    def extract_features(self, image, model_type):\n",
        "        \"\"\"Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ø§ Ù…Ø¯Ù„ Ù…Ø´Ø®Øµ\"\"\"\n",
        "        if isinstance(image, str):\n",
        "            image = Image.open(image).convert('RGB')\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if model_type == 'resnet50':\n",
        "                img_tensor = self.transforms['resnet50'](image).unsqueeze(0).to(self.device)\n",
        "                features = self.models['resnet50'](img_tensor).squeeze().cpu().numpy()\n",
        "\n",
        "            elif model_type == 'vit':\n",
        "                inputs = self.processors['vit'](images=image, return_tensors=\"pt\")\n",
        "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "                outputs = self.models['vit'](**inputs)\n",
        "                features = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "            elif model_type == 'clip':\n",
        "                inputs = self.processors['clip'](images=image, return_tensors=\"pt\")\n",
        "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "                features = self.models['clip'].get_image_features(**inputs).squeeze().cpu().numpy()\n",
        "\n",
        "        return features / np.linalg.norm(features) # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ\n",
        "\n",
        "    def add_similarity_text_to_image(self, image, similarity_score, rank=None):\n",
        "        \"\"\"Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…ØªÙ† Ø´Ø¨Ø§Ù‡Øª Ø±ÙˆÛŒ ØªØµÙˆÛŒØ±\"\"\"\n",
        "        img_copy = image.copy()\n",
        "        draw = ImageDraw.Draw(img_copy)\n",
        "\n",
        "        try:\n",
        "            font = ImageFont.load_default()\n",
        "        except:\n",
        "            font = ImageFont.load_default()\n",
        "\n",
        "        if rank:\n",
        "            text = f\"#{rank} - {similarity_score:.1%}\"\n",
        "        else:\n",
        "            text = f\"{similarity_score:.1%}\"\n",
        "\n",
        "        bbox = draw.textbbox((0, 0), text, font=font)\n",
        "        text_width = bbox[2] - bbox[0]\n",
        "        text_height = bbox[3] - bbox[1]\n",
        "\n",
        "        x = img_copy.width - text_width - 10\n",
        "        y = 10\n",
        "\n",
        "        padding = 5\n",
        "        draw.rectangle([\n",
        "            x - padding,\n",
        "            y - padding,\n",
        "            x + text_width + padding,\n",
        "            y + text_height + padding\n",
        "        ], fill=(0, 0, 0, 180))\n",
        "\n",
        "        draw.text((x, y), text, fill=(255, 255, 255), font=font)\n",
        "\n",
        "        return img_copy\n",
        "\n",
        "    def create_similarity_chart(self, results, query_type=\"ØªØµÙˆÛŒØ±ÛŒ\"):\n",
        "        \"\"\"Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆØ¯Ø§Ø± Ø´Ø¨Ø§Ù‡Øª\"\"\"\n",
        "        if not results:\n",
        "            return None\n",
        "\n",
        "        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª ÙØ§Ø±Ø³ÛŒ\n",
        "        plt.rcParams['font.family'] = ['DejaVu Sans']\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "        # Ù†Ù…ÙˆØ¯Ø§Ø± Ù…ÛŒÙ„Ù‡â€ŒØ§ÛŒ Ø´Ø¨Ø§Ù‡Øª\n",
        "        ranks = [f\"#{r['rank']}\" for r in results[:10]]\n",
        "        similarities = [r['similarity'] * 100 for r in results[:10]]\n",
        "\n",
        "        colors = plt.cm.viridis(np.linspace(0, 1, len(similarities)))\n",
        "        bars = ax1.bar(ranks, similarities, color=colors)\n",
        "\n",
        "        ax1.set_title(f'ğŸ“Š Ù…ÛŒØ²Ø§Ù† Ø´Ø¨Ø§Ù‡Øª - Ø¬Ø³ØªØ¬ÙˆÛŒ {query_type}', fontsize=14, fontweight='bold')\n",
        "        ax1.set_xlabel('Ø±ØªØ¨Ù‡ ØªØµØ§ÙˆÛŒØ±', fontsize=12)\n",
        "        ax1.set_ylabel('Ø¯Ø±ØµØ¯ Ø´Ø¨Ø§Ù‡Øª', fontsize=12)\n",
        "        ax1.set_ylim(0, 100)\n",
        "\n",
        "        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ø±ÙˆÛŒ Ù…ÛŒÙ„Ù‡â€ŒÙ‡Ø§\n",
        "        for bar, sim in zip(bars, similarities):\n",
        "            height = bar.get_height()\n",
        "            ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "                    f'{sim:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "        ax1.grid(axis='y', alpha=0.3)\n",
        "        ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # Ù†Ù…ÙˆØ¯Ø§Ø± Ø¯Ø§ÛŒØ±Ù‡â€ŒØ§ÛŒ ØªÙˆØ²ÛŒØ¹ Ø´Ø¨Ø§Ù‡Øª\n",
        "        similarity_ranges = {\n",
        "            'Ø¨Ø§Ù„Ø§ (80-100%)': sum(1 for s in similarities if s >= 80),\n",
        "            'Ù…ØªÙˆØ³Ø· (60-80%)': sum(1 for s in similarities if 60 <= s < 80),\n",
        "            'Ù¾Ø§ÛŒÛŒÙ† (40-60%)': sum(1 for s in similarities if 40 <= s < 60),\n",
        "            'Ø®ÛŒÙ„ÛŒ Ù¾Ø§ÛŒÛŒÙ† (<40%)': sum(1 for s in similarities if s < 40)\n",
        "        }\n",
        "\n",
        "        # Ø­Ø°Ù Ø¯Ø³ØªÙ‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ\n",
        "        similarity_ranges = {k: v for k, v in similarity_ranges.items() if v > 0}\n",
        "\n",
        "        if similarity_ranges:\n",
        "            colors_pie = ['#2E8B57', '#FFD700', '#FF6347', '#DC143C']\n",
        "            wedges, texts, autotexts = ax2.pie(\n",
        "                similarity_ranges.values(),\n",
        "                labels=similarity_ranges.keys(),\n",
        "                autopct='%1.1f%%',\n",
        "                colors=colors_pie[:len(similarity_ranges)],\n",
        "                startangle=90\n",
        "            )\n",
        "\n",
        "            ax2.set_title('ğŸ¯ ØªÙˆØ²ÛŒØ¹ Ù…ÛŒØ²Ø§Ù† Ø´Ø¨Ø§Ù‡Øª', fontsize=14, fontweight='bold')\n",
        "\n",
        "            # Ø¨Ù‡Ø¨ÙˆØ¯ Ø¸Ø§Ù‡Ø± Ù…ØªÙ†â€ŒÙ‡Ø§\n",
        "            for autotext in autotexts:\n",
        "                autotext.set_color('white')\n",
        "                autotext.set_fontweight('bold')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def create_model_comparison_chart(self, query_image, k=5):\n",
        "        \"\"\"Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§\"\"\"\n",
        "        if not query_image:\n",
        "            return None\n",
        "\n",
        "        plt.rcParams['font.family'] = ['DejaVu Sans']\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "        model_results = {}\n",
        "        colors = {'resnet50': '#FF6B6B', 'vit': '#4ECDC4', 'clip': '#45B7D1'}\n",
        "\n",
        "        for model_type in self.model_types:\n",
        "            results = self.search_by_image(query_image, model_type, k=k)\n",
        "            if results:\n",
        "                similarities = [r['similarity'] * 100 for r in results]\n",
        "                model_results[model_type.upper()] = similarities\n",
        "\n",
        "        # Ù†Ù…ÙˆØ¯Ø§Ø± Ø®Ø·ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡\n",
        "        x_pos = range(1, k + 1)\n",
        "\n",
        "        for model, sims in model_results.items():\n",
        "            ax.plot(x_pos[:len(sims)], sims,\n",
        "                   marker='o', linewidth=3, markersize=8,\n",
        "                   label=model, color=colors.get(model.lower(), '#333333'))\n",
        "\n",
        "            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ø±ÙˆÛŒ Ù†Ù‚Ø§Ø·\n",
        "            for i, sim in enumerate(sims):\n",
        "                ax.annotate(f'{sim:.1f}%',\n",
        "                           (i+1, sim),\n",
        "                           textcoords=\"offset points\",\n",
        "                           xytext=(0,10),\n",
        "                           ha='center', fontweight='bold')\n",
        "\n",
        "        ax.set_title('ğŸ“ˆ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§', fontsize=16, fontweight='bold')\n",
        "        ax.set_xlabel('Ø±ØªØ¨Ù‡ Ù†ØªØ§ÛŒØ¬', fontsize=12)\n",
        "        ax.set_ylabel('Ø¯Ø±ØµØ¯ Ø´Ø¨Ø§Ù‡Øª', fontsize=12)\n",
        "        ax.set_xticks(x_pos)\n",
        "        ax.set_xticklabels([f'#{i}' for i in x_pos])\n",
        "        ax.legend(fontsize=12)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.set_ylim(0, 100)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def create_feature_space_visualization(self, query_image, model_type, k=30):\n",
        "        \"\"\"Ù†Ù…ÙˆØ¯Ø§Ø± Ú©Ø§Ù‡Ø´ Ø§Ø¨Ø¹Ø§Ø¯ Ùˆ Ù†Ù…Ø§ÛŒØ´ ÙØ¶Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ø§ t-SNE\"\"\"\n",
        "        if not query_image or model_type not in self.indices:\n",
        "            return None\n",
        "\n",
        "        plt.rcParams['font.family'] = ['DejaVu Sans']\n",
        "\n",
        "        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒ ØªØµÙˆÛŒØ± Ù¾Ø±Ø³â€ŒÙˆØ¬Ùˆ\n",
        "        query_features = self.extract_features(query_image, model_type)\n",
        "\n",
        "        # Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² ØªØµØ§ÙˆÛŒØ± Ø¯ÛŒØªØ§Ø¨ÛŒØ³\n",
        "        sample_size = min(k, len(self.image_paths))\n",
        "        sample_indices = np.random.choice(len(self.image_paths), sample_size, replace=False)\n",
        "\n",
        "        # ØªØ±Ú©ÛŒØ¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø³â€ŒÙˆØ¬Ùˆ Ø¨Ø§ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§\n",
        "        features_to_plot = [query_features]\n",
        "        for idx in sample_indices:\n",
        "            features_to_plot.append(self.features[model_type][idx])\n",
        "\n",
        "        # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø¢Ø±Ø§ÛŒÙ‡\n",
        "        features_array = np.array(features_to_plot)\n",
        "\n",
        "        # Ú©Ø§Ù‡Ø´ Ø§Ø¨Ø¹Ø§Ø¯ Ø¨Ø§ t-SNE\n",
        "        tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(features_array)-1))\n",
        "        embedded_features = tsne.fit_transform(features_array)\n",
        "\n",
        "        # Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆØ¯Ø§Ø±\n",
        "        fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "        # Ø±Ù†Ú¯â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù†Ù‚Ø§Ø·\n",
        "        colors = ['#FF5733', '#C70039', '#900C3F', '#581845', '#2874A6',\n",
        "                 '#1ABC9C', '#F1C40F', '#27AE60', '#884EA0', '#D35400']\n",
        "\n",
        "        # Ù†Ù…Ø§ÛŒØ´ Ù†Ù‚Ø·Ù‡ Ù¾Ø±Ø³â€ŒÙˆØ¬Ùˆ\n",
        "        ax.scatter(embedded_features[0, 0], embedded_features[0, 1],\n",
        "                  s=150, c='red', marker='*', label='ØªØµÙˆÛŒØ± Ù¾Ø±Ø³â€ŒÙˆØ¬Ùˆ', zorder=10)\n",
        "\n",
        "        # Ù†Ù…Ø§ÛŒØ´ Ù†Ù‚Ø§Ø· Ø¯ÛŒØªØ§Ø¨ÛŒØ³\n",
        "        scatter = ax.scatter(embedded_features[1:, 0], embedded_features[1:, 1],\n",
        "                            s=80, c=np.arange(len(embedded_features)-1),\n",
        "                            cmap='viridis', alpha=0.7, label='ØªØµØ§ÙˆÛŒØ± Ø¯ÛŒØªØ§Ø¨ÛŒØ³')\n",
        "\n",
        "        # Ù†Ù…Ø§ÛŒØ´ Ø®Ø·ÙˆØ· Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ù†Ø²Ø¯ÛŒÚ©â€ŒØªØ±ÛŒÙ† Ù‡Ù…Ø³Ø§ÛŒÙ‡â€ŒÙ‡Ø§\n",
        "        distances = np.linalg.norm(embedded_features[1:] - embedded_features[0], axis=1)\n",
        "        closest_indices = np.argsort(distances)[:5] # 5 Ù‡Ù…Ø³Ø§ÛŒÙ‡ Ù†Ø²Ø¯ÛŒÚ©\n",
        "\n",
        "        for idx in closest_indices:\n",
        "            ax.plot([embedded_features[0, 0], embedded_features[idx+1, 0]],\n",
        "                   [embedded_features[0, 1], embedded_features[idx+1, 1]],\n",
        "                   'r--', alpha=0.4, linewidth=1)\n",
        "\n",
        "        # Ø§ÙØ²ÙˆØ¯Ù† Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§\n",
        "        ax.set_title(f'Ù†Ù…Ø§ÛŒØ´ ÙØ¶Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ø§ t-SNE ({model_type.upper()})', fontsize=14, fontweight='bold')\n",
        "        ax.set_xlabel('t-SNE Ø¨Ø¹Ø¯ Ø§ÙˆÙ„', fontsize=12)\n",
        "        ax.set_ylabel('t-SNE Ø¨Ø¹Ø¯ Ø¯ÙˆÙ…', fontsize=12)\n",
        "\n",
        "        # Ø§ÙØ²ÙˆØ¯Ù† Ø±Ø§Ù‡Ù†Ù…Ø§\n",
        "        ax.legend(loc='upper right', fontsize=10)\n",
        "\n",
        "        # Ø§ÙØ²ÙˆØ¯Ù† Ø±Ù†Ú¯â€ŒÙ†Ù…Ø§\n",
        "        cbar = plt.colorbar(scatter, ax=ax)\n",
        "        cbar.set_label('Ø´Ø§Ø®Øµ ØªØµÙˆÛŒØ±', fontsize=10)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def create_similarity_distribution_analysis(self, results):\n",
        "        \"\"\"ØªØ­Ù„ÛŒÙ„ ØªÙˆØ²ÛŒØ¹ Ø´Ø¨Ø§Ù‡Øª Ù†ØªØ§ÛŒØ¬\"\"\"\n",
        "        if not results:\n",
        "            return None\n",
        "\n",
        "        plt.rcParams['font.family'] = ['DejaVu Sans']\n",
        "\n",
        "        similarities = [r['similarity'] for r in results]\n",
        "\n",
        "        # Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆØ¯Ø§Ø± Ø¨Ø§ Ø¯Ùˆ Ø²ÛŒØ±Ù†Ù…ÙˆØ¯Ø§Ø±\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "        # 1. Ù†Ù…ÙˆØ¯Ø§Ø± ØªÙˆØ²ÛŒØ¹ (Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù…)\n",
        "        sns.histplot(similarities, bins=10, kde=True, ax=ax1, color='#3498db')\n",
        "        ax1.set_title('ØªÙˆØ²ÛŒØ¹ Ø´Ø¨Ø§Ù‡Øª Ù†ØªØ§ÛŒØ¬', fontsize=14, fontweight='bold')\n",
        "        ax1.set_xlabel('Ù…ÛŒØ²Ø§Ù† Ø´Ø¨Ø§Ù‡Øª', fontsize=12)\n",
        "        ax1.set_ylabel('ÙØ±Ø§ÙˆØ§Ù†ÛŒ', fontsize=12)\n",
        "\n",
        "        # Ø®Ø· Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†\n",
        "        mean_sim = np.mean(similarities)\n",
        "        ax1.axvline(mean_sim, color='r', linestyle='--', linewidth=2)\n",
        "        ax1.text(mean_sim, ax1.get_ylim()[1]*0.9, f'Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†: {mean_sim:.2f}',\n",
        "                color='r', fontweight='bold', ha='center', backgroundcolor='white')\n",
        "\n",
        "        # 2. Ù†Ù…ÙˆØ¯Ø§Ø± Ø¬Ø¹Ø¨Ù‡â€ŒØ§ÛŒ\n",
        "        sns.boxplot(y=similarities, ax=ax2, color='#2ecc71')\n",
        "        ax2.set_title('Ù†Ù…ÙˆØ¯Ø§Ø± Ø¬Ø¹Ø¨Ù‡â€ŒØ§ÛŒ Ø´Ø¨Ø§Ù‡Øª', fontsize=14, fontweight='bold')\n",
        "        ax2.set_ylabel('Ù…ÛŒØ²Ø§Ù† Ø´Ø¨Ø§Ù‡Øª', fontsize=12)\n",
        "\n",
        "        # Ø§ÙØ²ÙˆØ¯Ù† Ø¢Ù…Ø§Ø± ØªÙˆØµÛŒÙÛŒ\n",
        "        stats_text = (\n",
        "            f\"Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†: {np.mean(similarities):.3f}\\n\"\n",
        "            f\"Ù…ÛŒØ§Ù†Ù‡: {np.median(similarities):.3f}\\n\"\n",
        "            f\"Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø±: {np.std(similarities):.3f}\\n\"\n",
        "            f\"Ø­Ø¯Ø§Ù‚Ù„: {np.min(similarities):.3f}\\n\"\n",
        "            f\"Ø­Ø¯Ø§Ú©Ø«Ø±: {np.max(similarities):.3f}\"\n",
        "        )\n",
        "\n",
        "        ax2.text(0.05, 0.05, stats_text, transform=ax2.transAxes,\n",
        "                bbox=dict(facecolor='white', alpha=0.8, boxstyle='round,pad=0.5'),\n",
        "                fontsize=10, verticalalignment='bottom')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def create_similarity_heatmap(self, query_image, top_k=10):\n",
        "        \"\"\"Ù…Ø§ØªØ±ÛŒØ³ Ø­Ø±Ø§Ø±ØªÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø´Ø¨Ø§Ù‡Øª Ø¨ÛŒÙ† Ù…Ø¯Ù„â€ŒÙ‡Ø§\"\"\"\n",
        "        if not query_image or len(self.model_types) < 2:\n",
        "            return None\n",
        "\n",
        "        plt.rcParams['font.family'] = ['DejaVu Sans']\n",
        "\n",
        "        # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù†ØªØ§ÛŒØ¬ Ø§Ø² Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§\n",
        "        all_results = {}\n",
        "        for model_type in self.model_types:\n",
        "            results = self.search_by_image(query_image, model_type, k=top_k)\n",
        "            if results:\n",
        "                all_results[model_type] = results\n",
        "\n",
        "        if not all_results:\n",
        "            return None\n",
        "\n",
        "        # Ø³Ø§Ø®Øª Ù…Ø§ØªØ±ÛŒØ³ Ø´Ø¨Ø§Ù‡Øª Ø¨ÛŒÙ† Ù…Ø¯Ù„â€ŒÙ‡Ø§\n",
        "        model_names = list(all_results.keys())\n",
        "        similarity_matrix = np.zeros((len(model_names), len(model_names)))\n",
        "\n",
        "        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ²Ø§Ù† Ù‡Ù…Ù¾ÙˆØ´Ø§Ù†ÛŒ Ù†ØªØ§ÛŒØ¬ Ø¨ÛŒÙ† Ù…Ø¯Ù„â€ŒÙ‡Ø§\n",
        "        for i, model1 in enumerate(model_names):\n",
        "            results1 = all_results[model1]\n",
        "            paths1 = set(r['path'] for r in results1)\n",
        "\n",
        "            for j, model2 in enumerate(model_names):\n",
        "                results2 = all_results[model2]\n",
        "                paths2 = set(r['path'] for r in results2)\n",
        "\n",
        "                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø§Ø®Øµ Ø¬Ø§Ú©Ø§Ø±Ø¯ (Ù†Ø³Ø¨Øª Ø§Ø´ØªØ±Ø§Ú© Ø¨Ù‡ Ø§Ø¬ØªÙ…Ø§Ø¹)\n",
        "                intersection = len(paths1.intersection(paths2))\n",
        "                union = len(paths1.union(paths2))\n",
        "\n",
        "                if union > 0:\n",
        "                    similarity_matrix[i, j] = intersection / union\n",
        "                else:\n",
        "                    similarity_matrix[i, j] = 0\n",
        "\n",
        "        # Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆØ¯Ø§Ø± Ø­Ø±Ø§Ø±ØªÛŒ\n",
        "        fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "        # Ù†Ù…Ø§ÛŒØ´ Ù…Ø§ØªØ±ÛŒØ³ Ø­Ø±Ø§Ø±ØªÛŒ\n",
        "        im = ax.imshow(similarity_matrix, cmap='YlOrRd', vmin=0, vmax=1)\n",
        "\n",
        "        # ØªÙ†Ø¸ÛŒÙ… Ù…Ø­ÙˆØ±Ù‡Ø§\n",
        "        ax.set_xticks(np.arange(len(model_names)))\n",
        "        ax.set_yticks(np.arange(len(model_names)))\n",
        "        ax.set_xticklabels([m.upper() for m in model_names])\n",
        "        ax.set_yticklabels([m.upper() for m in model_names])\n",
        "\n",
        "        # Ú†Ø±Ø®Ø´ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ÛŒ Ù…Ø­ÙˆØ± Ø§ÙÙ‚ÛŒ\n",
        "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "        # Ù†Ù…Ø§ÛŒØ´ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¯Ø± Ù‡Ø± Ø®Ø§Ù†Ù‡\n",
        "        for i in range(len(model_names)):\n",
        "            for j in range(len(model_names)):\n",
        "                text = ax.text(j, i, f\"{similarity_matrix[i, j]:.2f}\",\n",
        "                              ha=\"center\", va=\"center\", color=\"black\" if similarity_matrix[i, j] < 0.7 else \"white\",\n",
        "                              fontweight=\"bold\")\n",
        "\n",
        "        # Ø¹Ù†ÙˆØ§Ù† Ùˆ Ù†ÙˆØ§Ø± Ø±Ù†Ú¯ÛŒ\n",
        "        ax.set_title(\"Ù…Ø§ØªØ±ÛŒØ³ Ù‡Ù…Ù¾ÙˆØ´Ø§Ù†ÛŒ Ù†ØªØ§ÛŒØ¬ Ø¨ÛŒÙ† Ù…Ø¯Ù„â€ŒÙ‡Ø§\", fontsize=14, fontweight='bold')\n",
        "        cbar = fig.colorbar(im, ax=ax)\n",
        "        cbar.set_label('Ù…ÛŒØ²Ø§Ù† Ù‡Ù…Ù¾ÙˆØ´Ø§Ù†ÛŒ (Ø´Ø§Ø®Øµ Ø¬Ø§Ú©Ø§Ø±Ø¯)', fontsize=12)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def create_confidence_analysis(self, results):\n",
        "        \"\"\"ØªØ­Ù„ÛŒÙ„ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ù†ØªØ§ÛŒØ¬\"\"\"\n",
        "        if not results or len(results) < 3:\n",
        "            return None\n",
        "\n",
        "        plt.rcParams['font.family'] = ['DejaVu Sans']\n",
        "\n",
        "        similarities = [r['similarity'] for r in results]\n",
        "        ranks = [r['rank'] for r in results]\n",
        "\n",
        "        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù†Ø±Ø® Ú©Ø§Ù‡Ø´ Ø´Ø¨Ø§Ù‡Øª\n",
        "        decay_rates = []\n",
        "        for i in range(1, len(similarities)):\n",
        "            if similarities[i-1] > 0: # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÙ‚Ø³ÛŒÙ… Ø¨Ø± ØµÙØ±\n",
        "                decay = (similarities[i-1] - similarities[i]) / similarities[i-1]\n",
        "                decay_rates.append(decay)\n",
        "\n",
        "        # Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆØ¯Ø§Ø±\n",
        "        fig, ax = plt.subplots(figsize=(12, 7))\n",
        "\n",
        "        # Ù†Ù…ÙˆØ¯Ø§Ø± Ø®Ø· Ø´Ø¨Ø§Ù‡Øª\n",
        "        ax.plot(ranks, similarities, 'o-', color='#3498db', linewidth=2, markersize=8, label='Ø´Ø¨Ø§Ù‡Øª')\n",
        "\n",
        "        # Ù†Ø§Ø­ÛŒÙ‡ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø¨Ø§Ù„Ø§\n",
        "        high_confidence_threshold = 0.75\n",
        "        ax.axhspan(high_confidence_threshold, 1.0, alpha=0.2, color='green', label=f'Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø¨Ø§Ù„Ø§ (>{high_confidence_threshold:.0%})')\n",
        "\n",
        "        # Ù†Ø§Ø­ÛŒÙ‡ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ù…ØªÙˆØ³Ø·\n",
        "        medium_confidence_threshold = 0.5\n",
        "        ax.axhspan(medium_confidence_threshold, high_confidence_threshold, alpha=0.2, color='yellow', label=f'Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ù…ØªÙˆØ³Ø· ({medium_confidence_threshold:.0%}-{high_confidence_threshold:.0%})')\n",
        "\n",
        "        # Ù†Ø§Ø­ÛŒÙ‡ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ù¾Ø§ÛŒÛŒÙ†\n",
        "        ax.axhspan(0, medium_confidence_threshold, alpha=0.2, color='red', label=f'Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ù¾Ø§ÛŒÛŒÙ† (<{medium_confidence_threshold:.0%})')\n",
        "\n",
        "        # ÛŒØ§ÙØªÙ† Ù†Ù‚Ø·Ù‡ Ø´Ú©Ø³Øª (Ø¬Ø§ÛŒÛŒ Ú©Ù‡ Ù†Ø±Ø® Ú©Ø§Ù‡Ø´ Ø¨ÛŒØ´ Ø§Ø² Ø¢Ø³ØªØ§Ù†Ù‡ Ø§Ø³Øª)\n",
        "        if decay_rates:\n",
        "            significant_drop_threshold = 0.2 # Ø¢Ø³ØªØ§Ù†Ù‡ Ø§ÙØª Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡\n",
        "            drop_points = [i+2 for i, rate in enumerate(decay_rates) if rate > significant_drop_threshold]\n",
        "\n",
        "            if drop_points:\n",
        "                first_drop = drop_points[0]\n",
        "                ax.axvline(x=first_drop, color='purple', linestyle='--', linewidth=2,\n",
        "                          label=f'Ù†Ù‚Ø·Ù‡ Ø´Ú©Ø³Øª (Ø±ØªØ¨Ù‡ {first_drop})')\n",
        "\n",
        "                # Ø§ÙØ²ÙˆØ¯Ù† Ù…ØªÙ† ØªÙˆØ¶ÛŒØ­\n",
        "                ax.text(first_drop + 0.1, ax.get_ylim()[1]*0.9,\n",
        "                       f'Ø§ÙØª Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ Ø¯Ø± Ø´Ø¨Ø§Ù‡Øª',\n",
        "                       color='purple', fontweight='bold')\n",
        "\n",
        "        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù†Ù…ÙˆØ¯Ø§Ø±\n",
        "        ax.set_title('ØªØ­Ù„ÛŒÙ„ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ù†ØªØ§ÛŒØ¬ Ø¬Ø³ØªØ¬Ùˆ', fontsize=14, fontweight='bold')\n",
        "        ax.set_xlabel('Ø±ØªØ¨Ù‡', fontsize=12)\n",
        "        ax.set_ylabel('Ù…ÛŒØ²Ø§Ù† Ø´Ø¨Ø§Ù‡Øª', fontsize=12)\n",
        "        ax.set_ylim(0, 1.05)\n",
        "        ax.set_xlim(0.5, len(similarities) + 0.5)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Ø§ÙØ²ÙˆØ¯Ù† Ø¨Ø±Ú†Ø³Ø¨ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø±ÙˆÛŒ Ù†Ù‚Ø§Ø·\n",
        "        for i, (x, y) in enumerate(zip(ranks, similarities)):\n",
        "            ax.annotate(f'{y:.2f}', (x, y), xytext=(0, 5),\n",
        "                       textcoords='offset points', ha='center', fontsize=8)\n",
        "\n",
        "        ax.legend(loc='lower left', fontsize=10)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        return fig\n",
        "\n",
        "    def build_indices(self, image_folder):\n",
        "        \"\"\"Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§\"\"\"\n",
        "        print(f\"ğŸ”¨ Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³â€ŒÙ‡Ø§...\")\n",
        "\n",
        "        self.image_paths = []\n",
        "        for filename in os.listdir(image_folder):\n",
        "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                self.image_paths.append(os.path.join(image_folder, filename))\n",
        "\n",
        "        print(f\"ğŸ“ ØªØ¹Ø¯Ø§Ø¯ ØªØµØ§ÙˆÛŒØ±: {len(self.image_paths)}\")\n",
        "\n",
        "        if len(self.image_paths) == 0:\n",
        "            return False\n",
        "\n",
        "        for model_type in self.model_types:\n",
        "            print(f\"\\nğŸ”§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø§ Ù…Ø¯Ù„ {model_type.upper()}...\")\n",
        "\n",
        "            features_list = []\n",
        "            for img_path in tqdm(self.image_paths, desc=f\"Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒ {model_type}\"):\n",
        "                try:\n",
        "                    features = self.extract_features(img_path, model_type)\n",
        "                    features_list.append(features)\n",
        "                except Exception as e:\n",
        "                    print(f\"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ {img_path}: {e}\")\n",
        "                    features_list.append(np.zeros(self.feature_dims[model_type]))\n",
        "\n",
        "            self.features[model_type] = np.array(features_list).astype('float32')\n",
        "\n",
        "            self.indices[model_type] = faiss.IndexFlatIP(self.feature_dims[model_type])\n",
        "            self.indices[model_type].add(self.features[model_type])\n",
        "\n",
        "            print(f\"âœ… Ø§ÛŒÙ†Ø¯Ú©Ø³ {model_type} Ø¨Ø§ {self.indices[model_type].ntotal} ØªØµÙˆÛŒØ± Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def search_by_image(self, query_image, model_type, k=10):\n",
        "        \"\"\"Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø§ ØªØµÙˆÛŒØ±\"\"\"\n",
        "        if model_type not in self.indices:\n",
        "            return []\n",
        "\n",
        "        query_features = self.extract_features(query_image, model_type)\n",
        "        query_features = query_features.reshape(1, -1).astype('float32')\n",
        "\n",
        "        distances, indices = self.indices[model_type].search(query_features, k)\n",
        "\n",
        "        results = []\n",
        "        for rank, (idx, dist) in enumerate(zip(indices[0], distances[0]), 1):\n",
        "            if idx < len(self.image_paths):\n",
        "                original_img = Image.open(self.image_paths[idx])\n",
        "                img_with_score = self.add_similarity_text_to_image(original_img, dist, rank)\n",
        "\n",
        "                results.append({\n",
        "                    'path': self.image_paths[idx],\n",
        "                    'similarity': float(dist),\n",
        "                    'model': model_type,\n",
        "                    'rank': rank,\n",
        "                    'image': img_with_score,\n",
        "                    'original_image': original_img\n",
        "                })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def search_by_text(self, text_query, k=10):\n",
        "        \"\"\"Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø§ Ù…ØªÙ† (ÙÙ‚Ø· CLIP)\"\"\"\n",
        "        if 'clip' not in self.indices:\n",
        "            return []\n",
        "\n",
        "        inputs = self.processors['clip'](text=[text_query], return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            text_features = self.models['clip'].get_text_features(**inputs)\n",
        "            text_features = text_features.squeeze().cpu().numpy()\n",
        "            text_features = text_features / np.linalg.norm(text_features)\n",
        "\n",
        "        text_features = text_features.reshape(1, -1).astype('float32')\n",
        "        distances, indices = self.indices['clip'].search(text_features, k)\n",
        "\n",
        "        results = []\n",
        "        for rank, (idx, dist) in enumerate(zip(indices[0], distances[0]), 1):\n",
        "            if idx < len(self.image_paths):\n",
        "                original_img = Image.open(self.image_paths[idx])\n",
        "                img_with_score = self.add_similarity_text_to_image(original_img, dist, rank)\n",
        "\n",
        "                results.append({\n",
        "                    'path': self.image_paths[idx],\n",
        "                    'similarity': float(dist),\n",
        "                    'model': 'clip',\n",
        "                    'rank': rank,\n",
        "                    'image': img_with_score,\n",
        "                    'original_image': original_img\n",
        "                })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def get_sample_images(self, count=6):\n",
        "        \"\"\"Ø¯Ø±ÛŒØ§ÙØª Ù†Ù…ÙˆÙ†Ù‡ ØªØµØ§ÙˆÛŒØ± Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³\"\"\"\n",
        "        if len(self.image_paths) == 0:\n",
        "            return []\n",
        "\n",
        "        sample_paths = random.sample(self.image_paths, min(count, len(self.image_paths)))\n",
        "        return sample_paths\n",
        "\n",
        "class CarImageDownloader:\n",
        "    \"\"\"Ø¯Ø§Ù†Ù„ÙˆØ¯ ØªØµØ§ÙˆÛŒØ± Ù…ØªÙ†ÙˆØ¹ Ø´Ø§Ù…Ù„ Ø®ÙˆØ¯Ø±Ùˆ\"\"\"\n",
        "\n",
        "    def __init__(self, base_dir='/content/image_database'):\n",
        "        self.base_dir = base_dir\n",
        "        os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "    def download_diverse_images(self, count=60):\n",
        "        \"\"\"Ø¯Ø§Ù†Ù„ÙˆØ¯ ØªØµØ§ÙˆÛŒØ± Ù…ØªÙ†ÙˆØ¹ Ø´Ø§Ù…Ù„ Ø®ÙˆØ¯Ø±Ùˆ\"\"\"\n",
        "        print(f\"ğŸ“¥ Ø¯Ø§Ù†Ù„ÙˆØ¯ {count} ØªØµÙˆÛŒØ± Ù…ØªÙ†ÙˆØ¹ Ø´Ø§Ù…Ù„ Ø®ÙˆØ¯Ø±Ùˆ...\")\n",
        "\n",
        "        downloaded = 0\n",
        "\n",
        "        for i in tqdm(range(count), desc=\"Ø¯Ø§Ù†Ù„ÙˆØ¯\"):\n",
        "            try:\n",
        "                url = f\"https://picsum.photos/400/400?random={i+2000}\"\n",
        "\n",
        "                response = requests.get(url, timeout=10)\n",
        "                if response.status_code == 200:\n",
        "                    img = Image.open(BytesIO(response.content))\n",
        "                    img = img.convert('RGB')\n",
        "\n",
        "                    filename = f\"img_{i:03d}.jpg\"\n",
        "                    filepath = os.path.join(self.base_dir, filename)\n",
        "                    img.save(filepath)\n",
        "                    downloaded += 1\n",
        "\n",
        "                time.sleep(0.1)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø§Ù†Ù„ÙˆØ¯ ØªØµÙˆÛŒØ± {i}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"âœ… {downloaded} ØªØµÙˆÛŒØ± Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯\")\n",
        "        return downloaded\n",
        "\n",
        "# Ù…ØªØºÛŒØ± Ø³Ø±Ø§Ø³Ø±ÛŒ\n",
        "search_engine = None\n",
        "\n",
        "def initialize_system():\n",
        "    \"\"\"Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ…\"\"\"\n",
        "    global search_engine\n",
        "\n",
        "    downloader = CarImageDownloader()\n",
        "    downloaded_count = downloader.download_diverse_images(count=60)\n",
        "\n",
        "    if downloaded_count == 0:\n",
        "        return False, \"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø§Ù†Ù„ÙˆØ¯ ØªØµØ§ÙˆÛŒØ±\"\n",
        "\n",
        "    search_engine = MultiModalImageSearchEngine(['resnet50', 'vit', 'clip'])\n",
        "\n",
        "    success = search_engine.build_indices('/content/image_database')\n",
        "\n",
        "    if success:\n",
        "        return True, f\"Ø³ÛŒØ³ØªÙ… Ø¨Ø§ {downloaded_count} ØªØµÙˆÛŒØ± Ùˆ 3 Ù…Ø¯Ù„ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª\"\n",
        "    else:\n",
        "        return False, \"Ø®Ø·Ø§ Ø¯Ø± Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³\"\n",
        "\n",
        "# Ø§Ø¬Ø±Ø§ÛŒ Ø³ÛŒØ³ØªÙ…\n",
        "print(\"ğŸš€ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…ÙˆØªÙˆØ± Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØµÙˆÛŒØ±ÛŒ Ú†Ù†Ø¯ Ù…Ø¯Ù„Ù‡...\")\n",
        "\n",
        "# Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ\n",
        "success, message = initialize_system()\n",
        "print(f\"ğŸ“Š {message}\")\n",
        "\n",
        "if success:\n",
        "    print(\"ğŸ¨ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ Gradio...\")\n",
        "\n",
        "    # Ø¯Ø±ÛŒØ§ÙØª Ù†Ù…ÙˆÙ†Ù‡ ØªØµØ§ÙˆÛŒØ±\n",
        "    sample_paths = search_engine.get_sample_images(6)\n",
        "    sample_images = []\n",
        "    for path in sample_paths:\n",
        "        try:\n",
        "            img = Image.open(path)\n",
        "            sample_images.append(img)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    def search_images_gradio(query_image, query_text, search_mode, model_type, num_results):\n",
        "        global search_engine\n",
        "\n",
        "        if search_engine is None:\n",
        "            return [], \"\", None, None, None, None, None\n",
        "\n",
        "        try:\n",
        "            if search_mode == \"Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø§ ØªØµÙˆÛŒØ±\":\n",
        "                if query_image is None:\n",
        "                    return [], \"Ù„Ø·ÙØ§Ù‹ ØªØµÙˆÛŒØ± Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯\", None, None, None, None, None\n",
        "                results = search_engine.search_by_image(query_image, model_type, k=num_results)\n",
        "                query_type = \"ØªØµÙˆÛŒØ±ÛŒ\"\n",
        "            else:\n",
        "                if not query_text:\n",
        "                    return [], \"Ù„Ø·ÙØ§Ù‹ Ù…ØªÙ† ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯\", None, None, None, None, None\n",
        "                if model_type != 'clip':\n",
        "                    return [], \"Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…ØªÙ†ÛŒ ÙÙ‚Ø· Ø¨Ø§ CLIP Ø§Ù…Ú©Ø§Ù†â€ŒÙ¾Ø°ÛŒØ± Ø§Ø³Øª\", None, None, None, None, None\n",
        "                results = search_engine.search_by_text(query_text, k=num_results)\n",
        "                query_type = \"Ù…ØªÙ†ÛŒ\"\n",
        "\n",
        "            if not results:\n",
        "                return [], \"Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯\", None, None, None, None, None\n",
        "\n",
        "            # ØªØµØ§ÙˆÛŒØ± Ø¨Ø§ Ù†Ù…Ø§ÛŒØ´ Ø´Ø¨Ø§Ù‡Øª\n",
        "            output_images = []\n",
        "            info_text = f\"ğŸ” {len(results)} Ù†ØªÛŒØ¬Ù‡ Ø¨Ø§ Ù…Ø¯Ù„ {model_type.upper()}:\\n\\n\"\n",
        "\n",
        "            for result in results:\n",
        "                img_with_score = result['image']\n",
        "                output_images.append(img_with_score)\n",
        "                info_text += f\"#{result['rank']}. Ø´Ø¨Ø§Ù‡Øª: {result['similarity']:.1%}\\n\"\n",
        "\n",
        "            # Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ÛŒ\n",
        "            similarity_chart = search_engine.create_similarity_chart(results, query_type)\n",
        "\n",
        "            # Ù†Ù…ÙˆØ¯Ø§Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ (ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØµÙˆÛŒØ±ÛŒ)\n",
        "            comparison_chart = None\n",
        "            if search_mode == \"Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø§ ØªØµÙˆÛŒØ±\" and query_image is not None:\n",
        "                comparison_chart = search_engine.create_model_comparison_chart(query_image, k=5)\n",
        "\n",
        "            # Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯\n",
        "            feature_space_viz = None\n",
        "            similarity_dist = None\n",
        "            confidence_analysis = None\n",
        "\n",
        "            if search_mode == \"Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø§ ØªØµÙˆÛŒØ±\" and query_image is not None:\n",
        "                # Ù†Ù…Ø§ÛŒØ´ ÙØ¶Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ\n",
        "                feature_space_viz = search_engine.create_feature_space_visualization(query_image, model_type)\n",
        "\n",
        "            # ØªØ­Ù„ÛŒÙ„ ØªÙˆØ²ÛŒØ¹ Ø´Ø¨Ø§Ù‡Øª Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ø§Ù†ÙˆØ§Ø¹ Ø¬Ø³ØªØ¬Ùˆ\n",
        "            similarity_dist = search_engine.create_similarity_distribution_analysis(results)\n",
        "\n",
        "            # ØªØ­Ù„ÛŒÙ„ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ù†ØªØ§ÛŒØ¬\n",
        "            confidence_analysis = search_engine.create_confidence_analysis(results)\n",
        "\n",
        "            return (output_images, info_text, similarity_chart, comparison_chart,\n",
        "                    feature_space_viz, similarity_dist, confidence_analysis)\n",
        "\n",
        "        except Exception as e:\n",
        "            return [], f\"Ø®Ø·Ø§: {str(e)}\", None, None, None, None, None\n",
        "\n",
        "    with gr.Blocks(title=\"Ù…ÙˆØªÙˆØ± Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØµÙˆÛŒØ±ÛŒ Ú†Ù†Ø¯ Ù…Ø¯Ù„Ù‡\", theme=gr.themes.Soft()) as demo:\n",
        "        gr.Markdown(\"\"\"\n",
        "        # ğŸ” Ù…ÙˆØªÙˆØ± Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØµÙˆÛŒØ±ÛŒ Ú†Ù†Ø¯ Ù…Ø¯Ù„Ù‡ Ø¨Ø§ ØªØ­Ù„ÛŒÙ„ Ù¾ÛŒØ´Ø±ÙØªÙ‡\n",
        "\n",
        "        ### Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø´Ø¯Ù‡:\n",
        "        - **ResNet50** (CNN): Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ú©Ø§Ù†ÙˆÙ„ÙˆØ´Ù†Ø§Ù„\n",
        "        - **ViT** (Vision Transformer): ØªØ±Ù†Ø³ÙÙˆØ±Ù…Ø± Ø¨ÛŒÙ†Ø§ÛŒÛŒ\n",
        "        - **CLIP**: Ù…Ø¯Ù„ Ú†Ù†Ø¯ÙˆØ¬Ù‡ÛŒ Ù…ØªÙ†-ØªØµÙˆÛŒØ±\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                search_mode = gr.Radio(\n",
        "                    [\"Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø§ ØªØµÙˆÛŒØ±\", \"Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø§ Ù…ØªÙ†\"],\n",
        "                    value=\"Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø§ ØªØµÙˆÛŒØ±\",\n",
        "                    label=\"Ù†ÙˆØ¹ Ø¬Ø³ØªØ¬Ùˆ\"\n",
        "                )\n",
        "\n",
        "                model_type = gr.Dropdown(\n",
        "                    choices=[\"resnet50\", \"vit\", \"clip\"],\n",
        "                    value=\"clip\",\n",
        "                    label=\"Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„\"\n",
        "                )\n",
        "\n",
        "                query_image = gr.Image(type=\"pil\", label=\"ØªØµÙˆÛŒØ± ÙˆØ±ÙˆØ¯ÛŒ\")\n",
        "                query_text = gr.Textbox(\n",
        "                    label=\"Ù…ØªÙ† Ø¬Ø³ØªØ¬Ùˆ\",\n",
        "                    placeholder=\"Ù…Ø«Ø§Ù„: red car, beautiful landscape, modern building\",\n",
        "                    visible=False\n",
        "                )\n",
        "\n",
        "                num_results = gr.Slider(1, 20, value=12, step=1, label=\"ØªØ¹Ø¯Ø§Ø¯ Ù†ØªØ§ÛŒØ¬\")\n",
        "                search_btn = gr.Button(\"ğŸ” Ø¬Ø³ØªØ¬Ùˆ\", variant=\"primary\")\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                output_gallery = gr.Gallery(\n",
        "                    label=\"Ù†ØªØ§ÛŒØ¬ Ø¬Ø³ØªØ¬Ùˆ (Ø¨Ø§ Ù†Ù…Ø§ÛŒØ´ Ù…ÛŒØ²Ø§Ù† Ø´Ø¨Ø§Ù‡Øª)\",\n",
        "                    columns=3,\n",
        "                    rows=4,\n",
        "                    height=\"auto\"\n",
        "                )\n",
        "\n",
        "                info_text = gr.Textbox(label=\"Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù†ØªØ§ÛŒØ¬\", lines=8)\n",
        "\n",
        "                # Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ÛŒ - ØªØ¨â€ŒØ¨Ù†Ø¯ÛŒ\n",
        "                with gr.Tabs():\n",
        "                    with gr.TabItem(\"ğŸ“Š Ù†Ù…ÙˆØ¯Ø§Ø± Ø´Ø¨Ø§Ù‡Øª\"):\n",
        "                        similarity_plot = gr.Plot(label=\"Ù†Ù…ÙˆØ¯Ø§Ø± Ø´Ø¨Ø§Ù‡Øª\")\n",
        "\n",
        "                    with gr.TabItem(\"ğŸ“ˆ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§\"):\n",
        "                        comparison_plot = gr.Plot(label=\"Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§\")\n",
        "\n",
        "                    with gr.TabItem(\"ğŸ”¬ ÙØ¶Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ\"):\n",
        "                        feature_space_plot = gr.Plot(label=\"Ù†Ù…Ø§ÛŒØ´ t-SNE ÙØ¶Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ\")\n",
        "\n",
        "                    with gr.TabItem(\"ğŸ“‰ ØªØ­Ù„ÛŒÙ„ ØªÙˆØ²ÛŒØ¹\"):\n",
        "                        similarity_dist_plot = gr.Plot(label=\"ØªØ­Ù„ÛŒÙ„ ØªÙˆØ²ÛŒØ¹ Ø´Ø¨Ø§Ù‡Øª\")\n",
        "\n",
        "                    with gr.TabItem(\"ğŸ¯ ØªØ­Ù„ÛŒÙ„ Ø§Ø·Ù…ÛŒÙ†Ø§Ù†\"):\n",
        "                        confidence_plot = gr.Plot(label=\"ØªØ­Ù„ÛŒÙ„ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ù†ØªØ§ÛŒØ¬\")\n",
        "\n",
        "        # ØªØºÛŒÛŒØ± Ù†Ù…Ø§ÛŒØ´\n",
        "        def update_visibility(mode):\n",
        "            if mode == \"Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø§ ØªØµÙˆÛŒØ±\":\n",
        "                return gr.update(visible=True), gr.update(visible=False)\n",
        "            else:\n",
        "                return gr.update(visible=False), gr.update(visible=True)\n",
        "\n",
        "        search_mode.change(\n",
        "            fn=update_visibility,\n",
        "            inputs=[search_mode],\n",
        "            outputs=[query_image, query_text]\n",
        "        )\n",
        "\n",
        "        # Ø§ØªØµØ§Ù„ Ø¬Ø³ØªØ¬Ùˆ\n",
        "        search_btn.click(\n",
        "            fn=search_images_gradio,\n",
        "            inputs=[query_image, query_text, search_mode, model_type, num_results],\n",
        "            outputs=[output_gallery, info_text, similarity_plot, comparison_plot,\n",
        "                    feature_space_plot, similarity_dist_plot, confidence_plot]\n",
        "        )\n",
        "\n",
        "        # Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ\n",
        "        gr.Markdown(\"### ğŸ“ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§:\")\n",
        "\n",
        "        # Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ ØªØµØ§ÙˆÛŒØ± ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³\n",
        "        examples_list = []\n",
        "\n",
        "        # Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ (ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ CLIP)\n",
        "        examples_list.extend([\n",
        "            [None, \"red sports car\", \"Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø§ Ù…ØªÙ†\", \"clip\", 8],\n",
        "            [None, \"beautiful sunset\", \"Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø§ Ù…ØªÙ†\", \"clip\", 8],\n",
        "            [None, \"mountain landscape\", \"Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø§ Ù…ØªÙ†\", \"clip\", 8],\n",
        "        ])\n",
        "\n",
        "        # Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ ØªØµÙˆÛŒØ±ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§\n",
        "        if len(sample_images) >= 3:\n",
        "            # ResNet50\n",
        "            examples_list.append([sample_images[0], \"\", \"Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø§ ØªØµÙˆÛŒØ±\", \"resnet50\", 8])\n",
        "            # ViT\n",
        "            examples_list.append([sample_images[1], \"\", \"Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø§ ØªØµÙˆÛŒØ±\", \"vit\", 8])\n",
        "            # CLIP\n",
        "            examples_list.append([sample_images[2], \"\", \"Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø§ ØªØµÙˆÛŒØ±\", \"clip\", 8])\n",
        "\n",
        "        gr.Examples(\n",
        "            examples=examples_list,\n",
        "            inputs=[query_image, query_text, search_mode, model_type, num_results],\n",
        "            label=\"Ú©Ù„ÛŒÚ© Ú©Ù†ÛŒØ¯ ØªØ§ Ø¬Ø³ØªØ¬Ùˆ Ø´ÙˆØ¯\"\n",
        "        )\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        ### ğŸ’¡ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡:\n",
        "\n",
        "        - **Ù†Ù…ÙˆØ¯Ø§Ø± Ø´Ø¨Ø§Ù‡Øª**: ØªÙˆØ²ÛŒØ¹ Ùˆ Ù…ÛŒØ²Ø§Ù† Ø´Ø¨Ø§Ù‡Øª Ù†ØªØ§ÛŒØ¬\n",
        "        - **Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§**: Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù (ÙÙ‚Ø· Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØµÙˆÛŒØ±ÛŒ)\n",
        "        - **ÙØ¶Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ**: Ù†Ù…Ø§ÛŒØ´ t-SNE Ø§Ø² ÙØ¶Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ Ùˆ Ù‡Ù…Ø³Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù†Ø²Ø¯ÛŒÚ©\n",
        "        - **ØªØ­Ù„ÛŒÙ„ ØªÙˆØ²ÛŒØ¹**: ØªÙˆØ²ÛŒØ¹ Ø¢Ù…Ø§Ø±ÛŒ Ø´Ø¨Ø§Ù‡Øªâ€ŒÙ‡Ø§ Ø¨Ø§ Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù… Ùˆ Ù†Ù…ÙˆØ¯Ø§Ø± Ø¬Ø¹Ø¨Ù‡â€ŒØ§ÛŒ\n",
        "        - **ØªØ­Ù„ÛŒÙ„ Ø§Ø·Ù…ÛŒÙ†Ø§Ù†**: Ø¨Ø±Ø±Ø³ÛŒ Ù†Ù‚Ø§Ø· Ø´Ú©Ø³Øª Ø¯Ø± Ø´Ø¨Ø§Ù‡Øª Ùˆ Ø³Ø·ÙˆØ­ Ø§Ø·Ù…ÛŒÙ†Ø§Ù†\n",
        "        \"\"\")\n",
        "\n",
        "    demo.launch(share=True, debug=True)\n",
        "    print(\"âœ… Ø³ÛŒØ³ØªÙ… Ú©Ø§Ù…Ù„ Ø¨Ø§ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª!\")\n",
        "    print(\"ğŸ“Š Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ø´Ø¨Ø§Ù‡ØªØŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ØŒ ÙØ¶Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒØŒ ØªÙˆØ²ÛŒØ¹ Ø´Ø¨Ø§Ù‡Øª Ùˆ ØªØ­Ù„ÛŒÙ„ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯\")\n",
        "else:\n",
        "    print(\"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SG_qY1ggVWWu",
        "outputId": "6857dee9-8206-4532-abbf-bcb991cf29d3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…ÙˆØªÙˆØ± Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØµÙˆÛŒØ±ÛŒ Ú†Ù†Ø¯ Ù…Ø¯Ù„Ù‡...\n",
            "ğŸ“¥ Ø¯Ø§Ù†Ù„ÙˆØ¯ 60 ØªØµÙˆÛŒØ± Ù…ØªÙ†ÙˆØ¹ Ø´Ø§Ù…Ù„ Ø®ÙˆØ¯Ø±Ùˆ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ø¯Ø§Ù†Ù„ÙˆØ¯: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:38<00:00,  1.56it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… 60 ØªØµÙˆÛŒØ± Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯\n",
            "ğŸš€ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±ÙˆÛŒ cuda...\n",
            "ğŸ“¦ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ResNet50...\n",
            "ğŸ“¦ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ViT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¦ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ CLIP...\n",
            "âœ… Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù†Ø¯!\n",
            "ğŸ”¨ Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³â€ŒÙ‡Ø§...\n",
            "ğŸ“ ØªØ¹Ø¯Ø§Ø¯ ØªØµØ§ÙˆÛŒØ±: 60\n",
            "\n",
            "ğŸ”§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø§ Ù…Ø¯Ù„ RESNET50...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒ resnet50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:00<00:00, 95.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Ø§ÛŒÙ†Ø¯Ú©Ø³ resnet50 Ø¨Ø§ 60 ØªØµÙˆÛŒØ± Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯\n",
            "\n",
            "ğŸ”§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø§ Ù…Ø¯Ù„ VIT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒ vit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:01<00:00, 56.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Ø§ÛŒÙ†Ø¯Ú©Ø³ vit Ø¨Ø§ 60 ØªØµÙˆÛŒØ± Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯\n",
            "\n",
            "ğŸ”§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø§ Ù…Ø¯Ù„ CLIP...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒ clip: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:00<00:00, 80.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Ø§ÛŒÙ†Ø¯Ú©Ø³ clip Ø¨Ø§ 60 ØªØµÙˆÛŒØ± Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯\n",
            "ğŸ“Š Ø³ÛŒØ³ØªÙ… Ø¨Ø§ 60 ØªØµÙˆÛŒØ± Ùˆ 3 Ù…Ø¯Ù„ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª\n",
            "ğŸ¨ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ Gradio...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://30154b2493f2aeafa8.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://30154b2493f2aeafa8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-2835459202.py:195: UserWarning: Glyph 128202 (\\N{BAR CHART}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-4-2835459202.py:195: UserWarning: Glyph 127919 (\\N{DIRECT HIT}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/tmp/ipython-input-4-2835459202.py:241: UserWarning: Glyph 128200 (\\N{CHART WITH UPWARDS TREND}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/processing_utils.py:158: UserWarning: Glyph 128202 (\\N{BAR CHART}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(output_bytes, format=fmt)\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/processing_utils.py:158: UserWarning: Glyph 127919 (\\N{DIRECT HIT}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(output_bytes, format=fmt)\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/processing_utils.py:158: UserWarning: Glyph 128200 (\\N{CHART WITH UPWARDS TREND}) missing from font(s) DejaVu Sans.\n",
            "  plt.savefig(output_bytes, format=fmt)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://43e49183ea598f1e29.gradio.live\n",
            "Killing tunnel 127.0.0.1:7861 <> https://991c85f6ffd4fc1071.gradio.live\n",
            "Killing tunnel 127.0.0.1:7862 <> https://30154b2493f2aeafa8.gradio.live\n",
            "âœ… Ø³ÛŒØ³ØªÙ… Ú©Ø§Ù…Ù„ Ø¨Ø§ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª!\n",
            "ğŸ“Š Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ø´Ø¨Ø§Ù‡ØªØŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ØŒ ÙØ¶Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒØŒ ØªÙˆØ²ÛŒØ¹ Ø´Ø¨Ø§Ù‡Øª Ùˆ ØªØ­Ù„ÛŒÙ„ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/events.py:89: UserWarning: Glyph 128202 (\\N{BAR CHART}) missing from font(s) DejaVu Sans.\n",
            "  func(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/events.py:89: UserWarning: Glyph 127919 (\\N{DIRECT HIT}) missing from font(s) DejaVu Sans.\n",
            "  func(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/events.py:89: UserWarning: Glyph 128200 (\\N{CHART WITH UPWARDS TREND}) missing from font(s) DejaVu Sans.\n",
            "  func(*args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BASED ON GOOGLE IMAGE**"
      ],
      "metadata": {
        "id": "yUknfpocg0kG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ù†ØµØ¨ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²\n",
        "!pip install -q faiss-cpu torch torchvision transformers gradio pillow requests tqdm\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import faiss\n",
        "import gradio as gr\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import random\n",
        "\n",
        "# Import models\n",
        "from torchvision import models, transforms\n",
        "from transformers import CLIPModel, CLIPProcessor, ViTModel, ViTImageProcessor\n",
        "\n",
        "class ImageSearchEngine:\n",
        "    \"\"\"Ù…ÙˆØªÙˆØ± Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØµÙˆÛŒØ±ÛŒ Ø³Ø§Ø¯Ù‡\"\"\"\n",
        "\n",
        "    def __init__(self, model_types=['resnet50', 'vit', 'clip']):\n",
        "        self.model_types = model_types\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.models = {}\n",
        "        self.processors = {}\n",
        "        self.transforms = {}\n",
        "        self.feature_dims = {}\n",
        "        self.indices = {}\n",
        "        self.image_paths = []\n",
        "        self.features = {}\n",
        "\n",
        "        print(f\"ğŸš€ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±ÙˆÛŒ {self.device}...\")\n",
        "        self._load_all_models()\n",
        "\n",
        "    def _load_all_models(self):\n",
        "        \"\"\"Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§\"\"\"\n",
        "\n",
        "        # 1. ResNet50 (CNN)\n",
        "        if 'resnet50' in self.model_types:\n",
        "            print(\"ğŸ“¦ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ResNet50...\")\n",
        "            self.models['resnet50'] = models.resnet50(pretrained=True)\n",
        "            self.models['resnet50'] = torch.nn.Sequential(*list(self.models['resnet50'].children())[:-1])\n",
        "            self.models['resnet50'].to(self.device)\n",
        "            self.models['resnet50'].eval()\n",
        "            self.feature_dims['resnet50'] = 2048\n",
        "\n",
        "            self.transforms['resnet50'] = transforms.Compose([\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                    std=[0.229, 0.224, 0.225])\n",
        "            ])\n",
        "\n",
        "        # 2. Vision Transformer (ViT)\n",
        "        if 'vit' in self.model_types:\n",
        "            print(\"ğŸ“¦ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ViT...\")\n",
        "            self.processors['vit'] = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
        "            self.models['vit'] = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
        "            self.models['vit'].to(self.device)\n",
        "            self.models['vit'].eval()\n",
        "            self.feature_dims['vit'] = 768\n",
        "\n",
        "        # 3. CLIP\n",
        "        if 'clip' in self.model_types:\n",
        "            print(\"ğŸ“¦ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ CLIP...\")\n",
        "            self.models['clip'] = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "            self.processors['clip'] = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "            self.models['clip'].to(self.device)\n",
        "            self.models['clip'].eval()\n",
        "            self.feature_dims['clip'] = 512\n",
        "\n",
        "        print(\"âœ… Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù†Ø¯!\")\n",
        "\n",
        "    def extract_features(self, image, model_type):\n",
        "        \"\"\"Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ø§ Ù…Ø¯Ù„ Ù…Ø´Ø®Øµ\"\"\"\n",
        "        if isinstance(image, str):\n",
        "            image = Image.open(image).convert('RGB')\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if model_type == 'resnet50':\n",
        "                img_tensor = self.transforms['resnet50'](image).unsqueeze(0).to(self.device)\n",
        "                features = self.models['resnet50'](img_tensor).squeeze().cpu().numpy()\n",
        "\n",
        "            elif model_type == 'vit':\n",
        "                inputs = self.processors['vit'](images=image, return_tensors=\"pt\")\n",
        "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "                outputs = self.models['vit'](**inputs)\n",
        "                features = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "            elif model_type == 'clip':\n",
        "                inputs = self.processors['clip'](images=image, return_tensors=\"pt\")\n",
        "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "                features = self.models['clip'].get_image_features(**inputs).squeeze().cpu().numpy()\n",
        "\n",
        "            return features / np.linalg.norm(features) # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ\n",
        "\n",
        "    def add_similarity_badge(self, image, similarity_score, rank=None):\n",
        "        \"\"\"Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù†Ø´Ø§Ù† Ø´Ø¨Ø§Ù‡Øª Ø¨Ù‡ ØªØµÙˆÛŒØ± (Ø´Ø¨ÛŒÙ‡ Ú¯ÙˆÚ¯Ù„)\"\"\"\n",
        "        img_copy = image.copy()\n",
        "        draw = ImageDraw.Draw(img_copy)\n",
        "\n",
        "        try:\n",
        "            font = ImageFont.load_default()\n",
        "        except:\n",
        "            font = ImageFont.load_default()\n",
        "\n",
        "        badge_text = f\"{similarity_score:.0%}\"\n",
        "\n",
        "        # Ù…ÙˆÙ‚Ø¹ÛŒØª Ù†Ø´Ø§Ù† - Ú¯ÙˆØ´Ù‡ Ø¨Ø§Ù„Ø§ Ø³Ù…Øª Ú†Ù¾\n",
        "        x, y = 10, 10\n",
        "\n",
        "        # Ú©Ø´ÛŒØ¯Ù† Ù…Ø³ØªØ·ÛŒÙ„ Ù†ÛŒÙ…Ù‡ Ø´ÙØ§Ù\n",
        "        draw.rectangle([x, y, x + 50, y + 20], fill=(0, 0, 0, 160))\n",
        "\n",
        "        # Ù…ØªÙ† ÙˆØ³Ø· Ù…Ø³ØªØ·ÛŒÙ„\n",
        "        draw.text((x + 5, y + 5), badge_text, fill=(255, 255, 255), font=font)\n",
        "\n",
        "        return img_copy\n",
        "\n",
        "    def build_indices(self, image_folder):\n",
        "        \"\"\"Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§\"\"\"\n",
        "        print(f\"ğŸ”¨ Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³â€ŒÙ‡Ø§...\")\n",
        "\n",
        "        self.image_paths = []\n",
        "        for filename in os.listdir(image_folder):\n",
        "            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                self.image_paths.append(os.path.join(image_folder, filename))\n",
        "\n",
        "        print(f\"ğŸ“ ØªØ¹Ø¯Ø§Ø¯ ØªØµØ§ÙˆÛŒØ±: {len(self.image_paths)}\")\n",
        "\n",
        "        if len(self.image_paths) == 0:\n",
        "            return False\n",
        "\n",
        "        for model_type in self.model_types:\n",
        "            print(f\"\\nğŸ”§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø§ Ù…Ø¯Ù„ {model_type.upper()}...\")\n",
        "\n",
        "            features_list = []\n",
        "            for img_path in tqdm(self.image_paths, desc=f\"Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒ {model_type}\"):\n",
        "                try:\n",
        "                    features = self.extract_features(img_path, model_type)\n",
        "                    features_list.append(features)\n",
        "                except Exception as e:\n",
        "                    print(f\"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ {img_path}: {e}\")\n",
        "                    features_list.append(np.zeros(self.feature_dims[model_type]))\n",
        "\n",
        "            self.features[model_type] = np.array(features_list).astype('float32')\n",
        "\n",
        "            self.indices[model_type] = faiss.IndexFlatIP(self.feature_dims[model_type])\n",
        "            self.indices[model_type].add(self.features[model_type])\n",
        "\n",
        "            print(f\"âœ… Ø§ÛŒÙ†Ø¯Ú©Ø³ {model_type} Ø¨Ø§ {self.indices[model_type].ntotal} ØªØµÙˆÛŒØ± Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def search_by_image(self, query_image, model_type, k=10):\n",
        "        \"\"\"Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø§ ØªØµÙˆÛŒØ±\"\"\"\n",
        "        if model_type not in self.indices:\n",
        "            return []\n",
        "\n",
        "        query_features = self.extract_features(query_image, model_type)\n",
        "        query_features = query_features.reshape(1, -1).astype('float32')\n",
        "\n",
        "        distances, indices = self.indices[model_type].search(query_features, k)\n",
        "\n",
        "        results = []\n",
        "        for rank, (idx, dist) in enumerate(zip(indices[0], distances[0]), 1):\n",
        "            if idx < len(self.image_paths):\n",
        "                original_img = Image.open(self.image_paths[idx])\n",
        "                img_with_score = self.add_similarity_badge(original_img, dist, rank)\n",
        "\n",
        "                results.append({\n",
        "                    'path': self.image_paths[idx],\n",
        "                    'similarity': float(dist),\n",
        "                    'model': model_type,\n",
        "                    'rank': rank,\n",
        "                    'image': img_with_score\n",
        "                })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def search_by_text(self, text_query, k=10):\n",
        "        \"\"\"Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø§ Ù…ØªÙ† (ÙÙ‚Ø· CLIP)\"\"\"\n",
        "        if 'clip' not in self.indices:\n",
        "            return []\n",
        "\n",
        "        inputs = self.processors['clip'](text=[text_query], return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            text_features = self.models['clip'].get_text_features(**inputs)\n",
        "            text_features = text_features.squeeze().cpu().numpy()\n",
        "            text_features = text_features / np.linalg.norm(text_features)\n",
        "\n",
        "            text_features = text_features.reshape(1, -1).astype('float32')\n",
        "            distances, indices = self.indices['clip'].search(text_features, k)\n",
        "\n",
        "            results = []\n",
        "            for rank, (idx, dist) in enumerate(zip(indices[0], distances[0]), 1):\n",
        "                if idx < len(self.image_paths):\n",
        "                    original_img = Image.open(self.image_paths[idx])\n",
        "                    img_with_score = self.add_similarity_badge(original_img, dist, rank)\n",
        "\n",
        "                    results.append({\n",
        "                        'path': self.image_paths[idx],\n",
        "                        'similarity': float(dist),\n",
        "                        'model': 'clip',\n",
        "                        'rank': rank,\n",
        "                        'image': img_with_score\n",
        "                    })\n",
        "\n",
        "            return results\n",
        "\n",
        "    def get_sample_images(self, count=6):\n",
        "        \"\"\"Ø¯Ø±ÛŒØ§ÙØª Ù†Ù…ÙˆÙ†Ù‡ ØªØµØ§ÙˆÛŒØ± Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³\"\"\"\n",
        "        if len(self.image_paths) == 0:\n",
        "            return []\n",
        "\n",
        "        sample_paths = random.sample(self.image_paths, min(count, len(self.image_paths)))\n",
        "        return sample_paths\n",
        "\n",
        "def download_images(base_dir='/content/image_database', count=100):\n",
        "    \"\"\"Ø¯Ø§Ù†Ù„ÙˆØ¯ ØªØµØ§ÙˆÛŒØ± Ù…ØªÙ†ÙˆØ¹\"\"\"\n",
        "    os.makedirs(base_dir, exist_ok=True)\n",
        "    print(f\"ğŸ“¥ Ø¯Ø§Ù†Ù„ÙˆØ¯ {count} ØªØµÙˆÛŒØ± Ù…ØªÙ†ÙˆØ¹...\")\n",
        "\n",
        "    downloaded = 0\n",
        "\n",
        "    # Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø§Ø² Picsum Photos\n",
        "    for i in tqdm(range(count), desc=\"Ø¯Ø§Ù†Ù„ÙˆØ¯\"):\n",
        "        try:\n",
        "            url = f\"https://picsum.photos/500/400?random={i+2000}\"\n",
        "\n",
        "            response = requests.get(url, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                img = Image.open(BytesIO(response.content))\n",
        "                img = img.convert('RGB')\n",
        "\n",
        "                filename = f\"img_{i:03d}.jpg\"\n",
        "                filepath = os.path.join(base_dir, filename)\n",
        "                img.save(filepath)\n",
        "                downloaded += 1\n",
        "\n",
        "                time.sleep(0.1)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø§Ù†Ù„ÙˆØ¯ ØªØµÙˆÛŒØ± {i}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"âœ… {downloaded} ØªØµÙˆÛŒØ± Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯\")\n",
        "    return downloaded\n",
        "\n",
        "# Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø³ÛŒØ³ØªÙ…\n",
        "print(\"ğŸš€ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…ÙˆØªÙˆØ± Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØµÙˆÛŒØ±ÛŒ Ø´Ø¨ÛŒÙ‡ Ú¯ÙˆÚ¯Ù„...\")\n",
        "\n",
        "# Ø¯Ø§Ù†Ù„ÙˆØ¯ ØªØµØ§ÙˆÛŒØ±\n",
        "downloaded_count = download_images(count=50)\n",
        "\n",
        "if downloaded_count > 0:\n",
        "    # Ø§ÛŒØ¬Ø§Ø¯ Ù…ÙˆØªÙˆØ± Ø¬Ø³ØªØ¬Ùˆ\n",
        "    search_engine = ImageSearchEngine(['resnet50', 'vit', 'clip'])\n",
        "\n",
        "    # Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³â€ŒÙ‡Ø§\n",
        "    success = search_engine.build_indices('/content/image_database')\n",
        "\n",
        "    if success:\n",
        "        print(\"ğŸ¨ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø´Ø¨ÛŒÙ‡ Ú¯ÙˆÚ¯Ù„...\")\n",
        "\n",
        "        # Ø¯Ø±ÛŒØ§ÙØª Ù†Ù…ÙˆÙ†Ù‡ ØªØµØ§ÙˆÛŒØ±\n",
        "        sample_paths = search_engine.get_sample_images(4)\n",
        "        sample_images = []\n",
        "        for path in sample_paths:\n",
        "            try:\n",
        "                img = Image.open(path)\n",
        "                sample_images.append(img)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        # ØªØ§Ø¨Ø¹ Ø¬Ø³ØªØ¬Ùˆ\n",
        "        def search_images(query_image, query_text, search_mode, model_type, num_results):\n",
        "            try:\n",
        "                if search_mode == \"ØªØµÙˆÛŒØ±\":\n",
        "                    if query_image is None:\n",
        "                        return [], \"Ù„Ø·ÙØ§Ù‹ ØªØµÙˆÛŒØ± Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯\"\n",
        "                    results = search_engine.search_by_image(query_image, model_type, k=num_results)\n",
        "                else:\n",
        "                    if not query_text:\n",
        "                        return [], \"Ù„Ø·ÙØ§Ù‹ Ù…ØªÙ† Ø¬Ø³ØªØ¬Ùˆ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯\"\n",
        "                    if model_type != 'clip':\n",
        "                        return [], \"Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…ØªÙ†ÛŒ ÙÙ‚Ø· Ø¨Ø§ CLIP Ø§Ù…Ú©Ø§Ù†â€ŒÙ¾Ø°ÛŒØ± Ø§Ø³Øª\"\n",
        "                    results = search_engine.search_by_text(query_text, k=num_results)\n",
        "\n",
        "                if not results:\n",
        "                    return [], \"Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯\"\n",
        "\n",
        "                # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø®Ø±ÙˆØ¬ÛŒ\n",
        "                output_images = [r['image'] for r in results]\n",
        "\n",
        "                # Ù…ØªÙ† Ù†ØªØ§ÛŒØ¬\n",
        "                info_text = f\"Ø­Ø¯ÙˆØ¯ {len(results)} Ù†ØªÛŒØ¬Ù‡ Ø¨Ø§ {model_type.upper()} | \"\n",
        "                info_text += f\"Ø¬Ø³ØªØ¬ÙˆÛŒ {'ØªØµÙˆÛŒØ±ÛŒ' if search_mode == 'ØªØµÙˆÛŒØ±' else 'Ù…ØªÙ†ÛŒ'}\"\n",
        "\n",
        "                return output_images, info_text\n",
        "\n",
        "            except Exception as e:\n",
        "                return [], f\"Ø®Ø·Ø§: {str(e)}\"\n",
        "\n",
        "        # Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ\n",
        "        with gr.Blocks(title=\"Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØµÙˆÛŒØ±ÛŒ Ø´Ø¨ÛŒÙ‡ Ú¯ÙˆÚ¯Ù„\") as demo:\n",
        "            gr.HTML(\"\"\"\n",
        "            <div style=\"display: flex; justify-content: center; margin-bottom: 20px;\">\n",
        "                <div style=\"font-size: 22px; font-weight: 500;\">\n",
        "                    <span style=\"color: #4285f4;\">I</span><span style=\"color: #ea4335;\">m</span><span style=\"color: #fbbc05;\">a</span><span style=\"color: #4285f4;\">g</span><span style=\"color: #34a853;\">e</span>\n",
        "                    <span style=\"margin-left: 5px;\">Search</span>\n",
        "                </div>\n",
        "            </div>\n",
        "            \"\"\")\n",
        "\n",
        "            with gr.Row():\n",
        "                search_mode = gr.Radio(\n",
        "                    [\"Ù…ØªÙ†\", \"ØªØµÙˆÛŒØ±\"],\n",
        "                    value=\"Ù…ØªÙ†\",\n",
        "                    label=\"Ù†ÙˆØ¹ Ø¬Ø³ØªØ¬Ùˆ\"\n",
        "                )\n",
        "\n",
        "            with gr.Row():\n",
        "                query_text = gr.Textbox(\n",
        "                    label=\"\",\n",
        "                    placeholder=\"Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØµØ§ÙˆÛŒØ±...\",\n",
        "                    container=False\n",
        "                )\n",
        "\n",
        "                query_image = gr.Image(\n",
        "                    label=\"\",\n",
        "                    type=\"pil\",\n",
        "                    visible=False\n",
        "                )\n",
        "\n",
        "            with gr.Row():\n",
        "                search_btn = gr.Button(\"ğŸ” Ø¬Ø³ØªØ¬Ùˆ\")\n",
        "\n",
        "            with gr.Accordion(\"ØªÙ†Ø¸ÛŒÙ…Ø§Øª\", open=False):\n",
        "                model_type = gr.Dropdown(\n",
        "                    choices=[\"resnet50\", \"vit\", \"clip\"],\n",
        "                    value=\"clip\",\n",
        "                    label=\"Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„\"\n",
        "                )\n",
        "\n",
        "                num_results = gr.Slider(\n",
        "                    minimum=5,\n",
        "                    maximum=30,\n",
        "                    value=12,\n",
        "                    step=1,\n",
        "                    label=\"ØªØ¹Ø¯Ø§Ø¯ Ù†ØªØ§ÛŒØ¬\"\n",
        "                )\n",
        "\n",
        "            # Ù†Ù…Ø§ÛŒØ´ Ù…ØªÙ† Ù†ØªØ§ÛŒØ¬\n",
        "            result_info = gr.Textbox(\n",
        "                label=\"\",\n",
        "                lines=1\n",
        "            )\n",
        "\n",
        "            # Ú¯Ø±ÛŒØ¯ Ù†ØªØ§ÛŒØ¬\n",
        "            result_gallery = gr.Gallery(\n",
        "                label=\"Ù†ØªØ§ÛŒØ¬ Ø¬Ø³ØªØ¬Ùˆ\",\n",
        "                columns=4,\n",
        "                rows=3,\n",
        "                height=\"auto\",\n",
        "                object_fit=\"cover\"\n",
        "            )\n",
        "\n",
        "            # ØªØºÛŒÛŒØ± Ù†Ù…Ø§ÛŒØ´ Ø¨Ø±Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹ Ø¬Ø³ØªØ¬Ùˆ\n",
        "            def update_search_mode(mode):\n",
        "                if mode == \"Ù…ØªÙ†\":\n",
        "                    return gr.update(visible=True), gr.update(visible=False)\n",
        "                else:\n",
        "                    return gr.update(visible=False), gr.update(visible=True)\n",
        "\n",
        "            search_mode.change(\n",
        "                fn=update_search_mode,\n",
        "                inputs=[search_mode],\n",
        "                outputs=[query_text, query_image]\n",
        "            )\n",
        "\n",
        "            # Ø§ØªØµØ§Ù„ Ø¬Ø³ØªØ¬Ùˆ\n",
        "            search_btn.click(\n",
        "                fn=search_images,\n",
        "                inputs=[query_image, query_text, search_mode, model_type, num_results],\n",
        "                outputs=[result_gallery, result_info]\n",
        "            )\n",
        "\n",
        "            # Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ\n",
        "            gr.Markdown(\"### Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ:\")\n",
        "\n",
        "            # Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ\n",
        "            text_examples = [\n",
        "                \"red sports car\",\n",
        "                \"beautiful landscape with mountains\",\n",
        "                \"modern architecture building\",\n",
        "                \"sunset over ocean\",\n",
        "                \"luxury vehicle\"\n",
        "            ]\n",
        "\n",
        "            gr.Examples(\n",
        "                examples=[[ex] for ex in text_examples],\n",
        "                inputs=[query_text],\n",
        "                label=\"Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ\"\n",
        "            )\n",
        "\n",
        "            # Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ ØªØµÙˆÛŒØ±ÛŒ\n",
        "            if len(sample_images) > 0:\n",
        "                gr.Examples(\n",
        "                    examples=[[img] for img in sample_images],\n",
        "                    inputs=[query_image],\n",
        "                    label=\"Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ ØªØµÙˆÛŒØ±ÛŒ\"\n",
        "                )\n",
        "\n",
        "        demo.launch(share=True)\n",
        "        print(\"âœ… Ù…ÙˆØªÙˆØ± Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØµÙˆÛŒØ±ÛŒ Ø¨Ø§ Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø´Ø¨ÛŒÙ‡ Ú¯ÙˆÚ¯Ù„ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯!\")\n",
        "    else:\n",
        "        print(\"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³\")\n",
        "else:\n",
        "    print(\"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø§Ù†Ù„ÙˆØ¯ ØªØµØ§ÙˆÛŒØ±\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "t7In2MNikfaY",
        "outputId": "154d4341-3549-4bc3-b947-020ef6e6e968"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…ÙˆØªÙˆØ± Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØµÙˆÛŒØ±ÛŒ Ø´Ø¨ÛŒÙ‡ Ú¯ÙˆÚ¯Ù„...\n",
            "ğŸ“¥ Ø¯Ø§Ù†Ù„ÙˆØ¯ 50 ØªØµÙˆÛŒØ± Ù…ØªÙ†ÙˆØ¹...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ø¯Ø§Ù†Ù„ÙˆØ¯: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:48<00:00,  1.04it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… 46 ØªØµÙˆÛŒØ± Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯\n",
            "ğŸš€ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±ÙˆÛŒ cuda...\n",
            "ğŸ“¦ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ResNet50...\n",
            "ğŸ“¦ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ViT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¦ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ CLIP...\n",
            "âœ… Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù†Ø¯!\n",
            "ğŸ”¨ Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³â€ŒÙ‡Ø§...\n",
            "ğŸ“ ØªØ¹Ø¯Ø§Ø¯ ØªØµØ§ÙˆÛŒØ±: 50\n",
            "\n",
            "ğŸ”§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø§ Ù…Ø¯Ù„ RESNET50...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒ resnet50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 90.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Ø§ÛŒÙ†Ø¯Ú©Ø³ resnet50 Ø¨Ø§ 50 ØªØµÙˆÛŒØ± Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯\n",
            "\n",
            "ğŸ”§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø§ Ù…Ø¯Ù„ VIT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒ vit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 55.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Ø§ÛŒÙ†Ø¯Ú©Ø³ vit Ø¨Ø§ 50 ØªØµÙˆÛŒØ± Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯\n",
            "\n",
            "ğŸ”§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø§ Ù…Ø¯Ù„ CLIP...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒ clip: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 76.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Ø§ÛŒÙ†Ø¯Ú©Ø³ clip Ø¨Ø§ 50 ØªØµÙˆÛŒØ± Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯\n",
            "ğŸ¨ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø´Ø¨ÛŒÙ‡ Ú¯ÙˆÚ¯Ù„...\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://991c85f6ffd4fc1071.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://991c85f6ffd4fc1071.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Ù…ÙˆØªÙˆØ± Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØµÙˆÛŒØ±ÛŒ Ø¨Ø§ Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø´Ø¨ÛŒÙ‡ Ú¯ÙˆÚ¯Ù„ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯!\n"
          ]
        }
      ]
    }
  ]
}